---
title: "fs2-kafka introduction: lib for ADT-events, at-least-once consumption, idempotent writes and batches of offsets"
permalink: /scala/fs2-kafka-pt2/
excerpt: "FS2 Kafka is a great library that simplifies reasoning around Kafka by representing event flow as fs2 streams. In that part we'll take a look how to"
categories:
  - scala
tags:
  - scala
  - kafka
  - fs2-kafka
  - functional programming
classes: wide
---
FS2 Kafka is a great library that simplifies reasoning around Kafka by representing event flow as fs2 streams.
Once I started working with it, I found that there were few public "slow start" snippets. So I take a chance to contribute and make the initial tech investigation easier for the next person.
The goal of the post is to provide a template with thorough error handling.
High-level flow is the following:

<img alt="High-level flow" src="/assets/images/fs2/4.jpg" width="70%">

# Events
## Unbreakable rule

Having a single schema per topic would be great from a technical perspective. No diversity, just the same set of properties every time. It could be stated as best practice: avoid multiple types of messages in any abstract message queue.

But there is another rule which is even more unbreakable. **Any events that need to stay in a fixed order must go in the same topic**. This is a cite from [Should You Put Several Event Types in the Same Kafka Topic?](https://www.confluent.io/blog/put-several-event-types-kafka-topic/) by Martin Kleppmann.
Highly recommend it if you need to get more feeling about what can justify having different kinds of messages in a topic.
[Putting Several Event Types in the Same Topic â€“ Revisited](https://www.confluent.io/blog/multiple-event-types-in-the-same-kafka-topic/) unfolds the topic and shows how to use schema registries to formalize "one of" kind of schemas.

Schema registries are not the topic for today. Instead, our focus is on the notion of event flow. 
## Study case
Let us imagine that we have Kafka as a backend for event sourcing. Hence, our study-case messages will describe diverse subsequent updates of the state.

The simplest scenario is bank operations.
We can have two of them to have a proper context:
- withdrawal;
- replenishment.

The reasoning about the importance of order should be free of explanations. To be precise, we talk about debit accounts; withdrawal is possible only when a sufficient amount is available on the account.
Now, we can pay attention to the events themselves.

Any *account* operation should have a reference to an account.

Hence, we can fix that obligation on the root trait.
```scala
sealed trait AccountOperation {  
  def account: UUID  
}
```

And we have two operations that form our study-case ADT.
```scala
final case class WithdrawalOperation(
  account: UUID, 
  value: Long
) extends AccountOperation  
  
final case class ReplenishmentOperation(
  account: UUID, 
  value: Long
) extends AccountOperation
```

## ADT serialization

### circe-extras
Surprisingly, there are no well-established conventions regarding ADT serialization.
For instance, Circe [proposes](https://circe.github.io/circe/codecs/adt.html) usage of class names as discriminator values.
So, you are supposed to have a configuration like the following:
```scala
implicit val config = Configuration.default.withDiscriminator("operation_type")
```
The derived codec will form such JSONs.
```json
{ 
  "operation_type" : "WithdrawalOperation" , 
  "account": "123e4567-e89b-12d3-a456-426614174000",
  "value": 200 
}
```

My stance here is that class names cause multiple problems:
1. When communicating with other teams, you must negotiate patterns and things like event types. "We can't consume `some_event:v1` because we can't name class in that way" sounds non-professional.
2. Even if we work in terms of perfectly shaped Domain Driven Design methodology, there could be a demand for refactoring. And "don't dare to rename classes" is a severe limitation.
3. It is necessary to explicitly care about the "immutability" of class names. For instance, maintain an isolated project with tests that do (de)serialization round-trips. The thesis here is that control over non-changing class names requires additional attention.
4. Versioning becomes difficult. Let's assume that we need to enrich `WithdrawalOperation` with additional property. The class handles the same semantics, the name should be the same, but you can't rename the original `WithdrawalOperation` to `WithdrawalOperationV1` or `LegacyWithdrawalOperation`. The best solution here is to append `V1` suffix to every class name.

That's why I prefer to keep discriminator values as explicitly written constants.

### circe-tagged-adt-codec

[abdolence/circe-tagged-adt-codec](https://github.com/abdolence/circe-tagged-adt-codec) brings ability to specify event type as constant in annotations.
```scala 
sealed trait TestEvent

@JsonAdt("my-event-1") 
case class MyEvent1(anyYourField : String) 
```
Resulting JSON looks in the following way:
```scala
{
  "type" : "my-event-1",
  "anyYourField" : "my-data"
}
```

### self-crafted circe-discriminator-codec

I wanted to extend provided functionality with two specific features:

1) Encoding JSONs with snake_case.
2) Easy configuration of property name treated as a discriminator. I had two concerns regarding hard-coded `"type"`:
   - Just like the value of the discriminator, the property name could be the subject of discussion with third parties. Or we can demand bringing domain-specific naming.
   - Any universal name can collide with the property of an entity.

So, here is the fork: [antonkw/circe-discriminator-codec](https://github.com/antonkw/circe-discriminator-codec)
Feel free to play around with it, released artifact is `"io.github.antonkw" %% "circe-tagged-adt-codec" % "0.0.7-SNAPSHOT"`. It is located in the `snapshots` of in  `s01.oss.sonatype.org`.

Now, we put `JsonAdt` annotation with the target discriminator:
```scala  
@JsonAdt("withdrawal:v1")  
final case class WithdrawalOperation(
  account: UUID, 
  value: Long
) extends AccountOperation  
  
@JsonAdt("replenishment:v1")  
final case class ReplenishmentOperation(
  account: UUID, 
  value: Long
) extends AccountOperation
```

Finally, we add a decoder.
```scala
object AccountOperation extends AutoDerivation {  
  implicit val accountOperationDecoder: Decoder[AccountOperation] =  
    JsonTaggedAdtCodec.createDecoder("operation_type")   
}  
```

Let's give us a try and run the app with `AccountOperation`  as input type.

<details markdown="block">
<summary markdown="span">KafkaProducer snippet</summary>
I just put raw JSONs as string to show what is being supplied.
```scala
    val producerSettings =
      ProducerSettings[IO, String, String]
        .withBootstrapServers("localhost:29092")
        .withProperty("topic.creation.enable", "true")

    val produce = KafkaProducer
      .resource(producerSettings)
      .use(
        _.produce(
          ProducerRecords(
            List(
              ProducerRecord(
                "topic1",
                "key",
                """
                  |{
                  |  "operation_type" : "replenishment:v1" ,
                  |  "account": "123e4567-e89b-12d3-a456-426614174000",
                  |  "value": 200
                  |}
                  |""".stripMargin
              ),
              ProducerRecord(
                "topic1",
                "key",
                """
                  |{
                  | "operation_type" : "withdrawal:v1" ,
                  | "account": "123e4567-e89b-12d3-a456-426614174000",
                  | "value": 100
                  |}
                  |""".stripMargin
              )
            )
          )
        ).flatten
      )
```
</details>

<details markdown="block">
<summary markdown="span">KafkaConsumer snippets</summary>
I use `deserializer` I wrote in the previous [part](https://antonkw.github.io/scala/fs2-kafka-pt1/).
```scala
    def deserializer[Body: Decoder]: Deserializer[IO, Either[DeserializationError, Body]] =
      Deserializer
        .string[IO]
        .flatMap(rawBody =>
          io.circe.parser
            .parse(rawBody)
            .fold(
              error => GenericDeserializer.fail[IO, Json](InvalidJson(rawBody, error)),
              GenericDeserializer.const[IO, Json]
            )
        )
        .flatMap(json =>
          json
            .as[Body]
            .fold(
              decodingFailure =>
                GenericDeserializer
                  .fail[IO, Body](InvalidEntity(json, decodingFailure)),
              GenericDeserializer.const[IO, Body]
            )
        )
        .attempt
        .map(_.left.map {
          case expected: DeserializationError => expected
          case unexpected                     => UnexpectedError(unexpected)
        })
```

Now, we derive `ConsumerSettings`
```scala
type Input = AccountOperation

implicit val accountOperationDeserializer: 
  Deserializer[IO, Either[DeserializationError, Input]] =
    deserializer[Input]
    
val consumerSettings =
  ConsumerSettings[IO, String, Either[DeserializationError, Input]]
    .withAutoOffsetReset(AutoOffsetReset.Earliest)
    .withBootstrapServers("localhost:29092")
    .withGroupId("group")
```

Finally, `KafkaConsumer` is in working state.
```scala
val stream: fs2.Stream[IO, Unit] =  
  KafkaConsumer  
    .stream(consumerSettings)  
    .subscribeTo("topic1")  
    .records  
    .evalMap { committable =>  
      processRecord(committable.record)  
    }
```
</details>

We feed two operations:
```json
{
    "operation_type": "replenishment:v1",
    "account": "123e4567-e89b-12d3-a456-426614174000",
    "value": 200
},
{
    "operation_type": "withdrawal:v1",
    "account": "123e4567-e89b-12d3-a456-426614174000",
    "value": 100
}
```

Successfully decoded lines appear in the console.
```
ConsumerRecord(   
  Right(
    ReplenishmentOperation(
      123e4567-e89b-12d3-a456-426614174000,
      200)
    )
  )
)

ConsumerRecord(
  Right(
    WithdrawalOperation(
      123e4567-e89b-12d3-a456-426614174000,
      100
    )
  )
)
```

Quick recap.
1. Our decoding mechanism allows parsing ADTs according to specified discriminator values.
2. Kafka provides guarantees and preserves chronological order of operations. When we tune partitioning, we must remember the importance of ordering for accounts.  